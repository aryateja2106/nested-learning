{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Learning Quickstart (Notebook)\n",
    "\n",
    "A fast, notebook-friendly way to sanity-check the HOPE model. This runs a tiny forward/backward step and prints device info. For full training, use `train_hope.py` from the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies (optional)\n",
    "\n",
    "If your environment does not already have the project installed, uncomment the line below. Colab users can run it as-is; local users with `uv` can skip this and use the CLI instead.\n",
    "\n",
    "```bash\n",
    "# !pip install -q -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, sys, os\n",
    "\n",
    "# Make sure the repo root is on sys.path whether we start in root or notebooks/\n",
    "ROOT = pathlib.Path().resolve()\n",
    "if (ROOT / \"src\").exists():\n",
    "    sys.path.append(str(ROOT))\n",
    "elif (ROOT.parent / \"src\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "    sys.path.append(str(ROOT))\n",
    "else:\n",
    "    raise RuntimeError(\"Run this notebook from the repo root or the notebooks/ directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Using CPU (works for quick smoke tests)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.hope import Hope, HopeConfig\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Tiny config for quick runs; fits on CPU or small GPUs\n",
    "config = HopeConfig(\n",
    "    d_model=64,\n",
    "    d_hidden=256,\n",
    "    d_key=16,\n",
    "    d_value=16,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    vocab_size=256,\n",
    "    max_seq_len=128,\n",
    "    cms_num_levels=2,\n",
    "    cms_base_chunk_size=4,\n",
    ")\n",
    "\n",
    "model = Hope(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Dummy tokens\n",
    "batch_size, seq_len = 2, 16\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len), device=device)\n",
    "labels = torch.randint(0, config.vocab_size, (batch_size, seq_len), device=device)\n",
    "\n",
    "model.train()\n",
    "out = model(input_ids, labels=labels)\n",
    "loss = out[\"loss\"]\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Loss:\", float(loss))\n",
    "print(\"Logits shape:\", out[\"logits\"].shape)\n",
    "print(\"First token logits (5 dims):\", out[\"logits\"][0, 0, :5].detach().cpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
