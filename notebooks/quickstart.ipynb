{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Nested Learning Quickstart\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aryateja2106/nested-learning/blob/main/notebooks/quickstart.ipynb)\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to create and run the HOPE model\n",
    "- Forward and backward pass basics\n",
    "- Visualize the Continuum Memory System (CMS) update schedule\n",
    "\n",
    "**Time**: ~2 minutes (CPU) | ~30 seconds (GPU)\n",
    "\n",
    "> üí° **Tip**: For full training, use `train_hope.py` from the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Run this cell to install required packages. Skip if already installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy matplotlib gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Setup & Device Check\n",
    "\n",
    "Check your device and import the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo root to path\n",
    "repo_root = Path().resolve().parent if Path().resolve().name == \"notebooks\" else Path().resolve()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"   Using CPU (works fine for this demo)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66aa258",
   "metadata": {},
   "source": [
    "## Step 3: Hello World - Forward Pass\n",
    "\n",
    "Create a tiny HOPE model and run a forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.hope import Hope, HopeConfig\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Tiny config - fits on CPU or small GPUs\n",
    "config = HopeConfig(\n",
    "    d_model=64,\n",
    "    d_hidden=256,\n",
    "    d_key=16,\n",
    "    d_value=16,\n",
    "    num_heads=4,\n",
    "    num_layers=1,\n",
    "    vocab_size=256,\n",
    "    max_seq_len=128,\n",
    "    cms_num_levels=2,\n",
    "    cms_base_chunk_size=4,\n",
    ")\n",
    "\n",
    "model = Hope(config).to(device)\n",
    "print(f\"‚úÖ Model created: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Simple forward pass\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 16), device=device)\n",
    "labels = torch.randint(0, config.vocab_size, (2, 16), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, labels=labels)\n",
    "    print(f\"‚úÖ Forward pass successful!\")\n",
    "    print(f\"   Loss: {output['loss'].item():.4f}\")\n",
    "    print(f\"   Logits shape: {output['logits'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c99060",
   "metadata": {},
   "source": [
    "## Step 4: Mini Training Loop\n",
    "\n",
    "Run a quick training step to verify everything works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "model.train()\n",
    "output = model(input_ids, labels=labels)\n",
    "loss = output[\"loss\"]\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"‚úÖ Training step successful!\")\n",
    "print(f\"   Loss: {loss.item():.4f}\")\n",
    "print(f\"   ‚úì Forward pass\")\n",
    "print(f\"   ‚úì Backward pass\")\n",
    "print(f\"   ‚úì Optimizer step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2e506",
   "metadata": {},
   "source": [
    "## Step 5: Visualize CMS Update Schedule\n",
    "\n",
    "The Continuum Memory System updates different levels at different frequencies. Let's visualize this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c67bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get CMS update schedule\n",
    "num_levels = config.cms_num_levels\n",
    "base_chunk_size = config.cms_base_chunk_size\n",
    "total_steps = 64\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_levels))\n",
    "\n",
    "for level in range(num_levels):\n",
    "    chunk_size = base_chunk_size * (2 ** level)\n",
    "    update_steps = list(range(chunk_size, total_steps + 1, chunk_size))\n",
    "    \n",
    "    for step in update_steps:\n",
    "        ax.axvline(x=step, color=colors[level], alpha=0.7, linewidth=2)\n",
    "    \n",
    "    ax.plot([], [], color=colors[level], linewidth=2, \n",
    "            label=f'Level {level} (updates every {chunk_size} steps)')\n",
    "\n",
    "ax.set_xlim(0, total_steps)\n",
    "ax.set_xlabel('Training Step', fontsize=12)\n",
    "ax.set_ylabel('CMS Update Events', fontsize=12)\n",
    "ax.set_title('Continuum Memory System: Multi-Scale Update Schedule', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ CMS visualization complete!\")\n",
    "print(\"   High-frequency levels update often (fast adaptation)\")\n",
    "print(\"   Low-frequency levels update rarely (long-term memory)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412bfb6",
   "metadata": {},
   "source": [
    "## üéâ What's Next?\n",
    "\n",
    "**You've successfully run the HOPE model!** Here's what to explore next:\n",
    "\n",
    "### For Researchers & Developers\n",
    "\n",
    "1. **Full Training**: Use `train_hope.py` with configs (small/medium/large)\n",
    "   ```bash\n",
    "   python train_hope.py --config small --steps 500\n",
    "   ```\n",
    "\n",
    "2. **Interactive Demo**: Launch the Gradio demo\n",
    "   ```bash\n",
    "   python demo/app.py\n",
    "   ```\n",
    "\n",
    "3. **Explore Components**: Check out `src/core/optimizers.py` and `src/core/memory.py`\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- üìÑ **Paper**: [Nested Learning PDF](https://abehrouz.github.io/files/NL.pdf)\n",
    "- üìù **Blog**: [Google Research Blog](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/)\n",
    "- üìö **Algorithm Notes**: See `docs/ALGORITHMS.md`\n",
    "\n",
    "---\n",
    "\n",
    "**‚≠ê If this helped you, please star the repo!** It helps others discover this implementation.\n",
    "\n",
    "[GitHub Repository](https://github.com/aryateja2106/nested-learning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
